[ft_instance_hyperparameter]
max_batch_size=256 ; Use for allocate the buffer
max_seq_len=1500 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=1 ; k value for top k sampling
top_p=0 ; p value for top p sampling
temperature=1.0 ; Use for sampling
repetition_penalty=1.0 ; Use for sampling
pipeline_para_size=1
data_type=fp16
sparse=0
int8_mode=0
enable_custom_all_reduce=0
;
len_penalty=0.0
beam_search_diversity_rate=0.0
shared_contexts_ratio=0.0   ; disable the optimization of shared_contexts, otherwise the context input pre-processing will be very complicated here
return_log_probs=false   ; return the output log probs and cumulative log probs.
context_log_probs=false  ; include input contexts in the cumulative log probability computation.
remove_padding=false     ; must set to false, otherwise strange shape will be caused for SpMM ; [Haojun]

;!!!!!!!!!!!!!!!!!!!!!! Choosing your models here !!!!!!!!!!!!!!!!!!!!!!
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;Model Choosing
model_name=opt_13B
tensor_para_size=1
model_dir = /data2/fanruibo/models/opt-13b/ft-model-60/1-gpu
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
[request]
request_batch_size=16
request_output_len=128

[gpt_124M]
head_num=12
size_per_head=64
vocab_size=50257
decoder_layers=12
start_id=50256
end_id=50256
inter_size=3072
num_tasks=3 ;optional
prompt_learning_start_id=50257 ;optional
prompt_learning_type=3 ;optional

;prompt learning example (optional)
[gpt_124M_task_0] ; task_name_id = 0
task_name=sentiment
prompt_length=10
;optional
[gpt_124M_task_1] ; task_name_id = 1
task_name=intent_and_slot
prompt_length=10
;optional
[gpt_124M_task_2] ; task_name_id = 2
task_name=squad
prompt_length=16

[megatron_345M]
head_num=16
size_per_head=64
vocab_size=50304
decoder_layers=24
start_id=50256
end_id=50256
inter_size=4096

[megatron_1.3B_adapter]
head_num=32
size_per_head=64
vocab_size=50304
decoder_layers=24
start_id=50256
end_id=50256
inter_size=8192
layernorm_eps=1e-5
adapter_inter_size=1024
has_adapters=true

[megatron_6.7B]
head_num=32
size_per_head=128
vocab_size=51200
decoder_layers=32
start_id=50256
end_id=50256
inter_size=16384

[megatron_20B]
head_num=48
size_per_head=128
vocab_size=51200
decoder_layers=44
start_id=50256
end_id=50256
inter_size=24576

[gpt_175B]
head_num=96
size_per_head=128
vocab_size=51200
decoder_layers=96
start_id=50256
end_id=50256
inter_size=49152

[opt_125M]
head_num=12
size_per_head=64
vocab_size=50272
decoder_layers=12
start_id=2
end_id=2
inter_size=3072
model_variant=opt-pre ;define variant structure

[opt_350M]
head_num=16
size_per_head=64
vocab_size=50272
decoder_layers=24
start_id=2
end_id=2
inter_size=4096
model_variant=opt-post ;  special in whole OPT family

[opt_6.7B]
head_num=32
size_per_head=128
vocab_size=50272
decoder_layers=32
start_id=2
end_id=2
inter_size=16384
model_variant=opt-pre ;define variant structure

[opt_13B]
head_num=40
size_per_head=128
vocab_size=50272
decoder_layers=40
start_id=2
end_id=2
inter_size=20480
model_variant=opt-pre ;define variant structure

[opt_30B]
head_num=56
size_per_head=128
vocab_size=50272
decoder_layers=48
start_id=2
end_id=2
inter_size=28672
model_variant=opt-pre ;define variant structure

[opt_66B]
head_num=72
size_per_head=128
vocab_size=50272
decoder_layers=64
start_id=2
end_id=2
inter_size=36864
model_variant=opt-pre ;define variant structure

[opt_175B]
head_num=96
size_per_head=128
vocab_size=50272
decoder_layers=96
start_id=2
end_id=2
inter_size=49152
model_variant=opt-pre ;define variant structure

[self_defined]
head_num=16
size_per_head=64
vocab_size=30000
decoder_layers=12
start_id=50256
end_id=50256
inter_size=4096
